{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de05cf-8ebb-4ab6-9845-2eec5fb58582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diplomacy_gym_environment import DiplomacyEnvironment\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "\n",
    "# This is a template setup on how to actually use the Gym environment I made, by setting up a RL agent.\n",
    "\n",
    "\n",
    "def random_move():\n",
    "    actions = {}\n",
    "    for power_name in env.game.powers.keys():\n",
    "        actions[power_name] = [random.choice(env.game.get_all_possible_orders()[loc]) for loc in\n",
    "                               env.game.get_orderable_locations(power_name)]\n",
    "    return actions\n",
    "\n",
    "\n",
    "def random_nn_move():\n",
    "    actions = {}\n",
    "    for power_name in env.game.powers.keys():\n",
    "        actions[power_name] = np.array([random.random() for _ in env.action_list])\n",
    "    return actions\n",
    "\n",
    "\n",
    "def random_vs_nonrandom_move(probs):\n",
    "    actions = {}\n",
    "    for power_name in env.game.powers.keys():\n",
    "        if power_name == 'AUSTRIA':\n",
    "            actions[power_name] = np.array(probs)\n",
    "        else:\n",
    "            actions[power_name] = np.array([random.random() for _ in env.action_list])\n",
    "    return actions\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(num_inputs,))\n",
    "    common1 = layers.Dense(num_middle, activation=\"relu\")(inputs)\n",
    "    common2 = layers.Dense(num_middle, activation=\"relu\")(common1)\n",
    "    common3 = layers.Dense(num_middle, activation=\"relu\")(common2)\n",
    "    common4 = layers.Dense(num_middle, activation=\"relu\")(common3)\n",
    "    actor = layers.Dense(num_actions, activation=\"sigmoid\")(common4)\n",
    "    critic = layers.Dense(1)(common4)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "\n",
    "\n",
    "def visualize_state(saved_display, rendering):\n",
    "    # display actions committed state\n",
    "    saved_display.update(display.SVG(rendering))\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "    # display following state\n",
    "    rendering = env.render()\n",
    "    saved_display.update(display.SVG(rendering))\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "def choose_action(action_probs, epsilon):\n",
    "    roll = np.random.rand(1)[0]\n",
    "    if epsilon > roll:\n",
    "        # Take random action\n",
    "        action = random_nn_move()\n",
    "    else:\n",
    "        # Take best action\n",
    "        #action = tf.argmax(action_probs[0]).numpy()\n",
    "        action = random_vs_nonrandom_move(action_probs[0])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e19d6-9b71-4b6e-8d78-d7a989a8c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own custom-made gym environment\n",
    "env = DiplomacyEnvironment(prints=False, render_path=None)\n",
    "\n",
    "# various other settings\n",
    "seed = 42\n",
    "wait_time = 0.1\n",
    "print_level = 0\n",
    "visualize_every = 10\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# setting Epsilon (exploration vs. exploitation parameter)\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "# number of frames to go from epsilon_min to epsilon_max\n",
    "epsilon_greedy_episodes = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "# size of NN layers\n",
    "num_inputs = env.observation_space.n\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_middle = 1024\n",
    "\n",
    "\n",
    "if print_level >= 1:\n",
    "    print(f'input layer size: {num_inputs}')\n",
    "    print(f'4 x middle layer size: {num_middle}')\n",
    "    print(f'output layer size: {num_actions}')\n",
    "\n",
    "# defining learning model\n",
    "model = create_model()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "# stats to keep track of\n",
    "action_history = []\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "\n",
    "#keep track of how healthy the network is\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "total_frame_count = 0\n",
    "\n",
    "# not sure about these values, has to do with batching, experience replay and not updating the network after every episode / step\n",
    "'''\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 100\n",
    "'''\n",
    "\n",
    "saved_display = display.display(\"This text should be replaced by rendering...\", display_id=True)\n",
    "\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        # this is the start of an episode\n",
    "        done = False\n",
    "        # setting to use highest action every time rather than probability distribution\n",
    "        # currently exploiter mode is the default and probability mode has not been implemented so this setting does nothing yet.\n",
    "        exploiter = False\n",
    "        state = np.array(env.reset())\n",
    "        episode_reward = 0\n",
    "        frame_count = 0\n",
    "\n",
    "        with tqdm(desc=f\"episode {episode_count} steps\") as pbar:\n",
    "            while not done:\n",
    "                # this is the start of a frame\n",
    "                pbar.update(1)\n",
    "\n",
    "                # convert state to tensor\n",
    "                state_tensor = tf.convert_to_tensor(state)\n",
    "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "\n",
    "                # Use network\n",
    "                action_probs, critic_value = model(state_tensor, training=False)\n",
    "\n",
    "                # Coose action based on policy\n",
    "                action = choose_action(action_probs, epsilon)\n",
    "                # TODO at a certain print level print level translate action back to common names and print them as well as their probability\n",
    "\n",
    "                # Apply the sampled action in our environment\n",
    "                state_next, reward, done, info, rendering = env.step(action, render=True)\n",
    "                # TODO at a certain print level print expected value, reward, and expected value of next_state as well as the relations between these\n",
    "\n",
    "                state_name, info = info\n",
    "                episode_reward += reward[0]\n",
    "                reward = reward[0]\n",
    "                done = done[0]\n",
    "                state_next = np.array(state_next[0])\n",
    "                action_mask = np.zeros(num_actions)\n",
    "                action_mask[info['AUSTRIA']] = 1\n",
    "\n",
    "                if episode_count % visualize_every == 0:\n",
    "                    visualize_state(saved_display, rendering)\n",
    "\n",
    "                # Save stats in replay buffer\n",
    "                action_history.append(action_mask)\n",
    "                action_probs_history.append(action['AUSTRIA'])\n",
    "                critic_value_history.append(critic_value[0, 0])\n",
    "                state_history.append(state)\n",
    "                state_next_history.append(state_next)\n",
    "                done_history.append(done)\n",
    "                rewards_history.append(reward)\n",
    "\n",
    "                # Change state\n",
    "                state = state_next\n",
    "\n",
    "                # this is the end of a frame\n",
    "                frame_count += 1\n",
    "                total_frame_count += 1\n",
    "\n",
    "            # start of end of episode here\n",
    "            pbar.close()\n",
    "\n",
    "            # Update running reward to check condition for solving\n",
    "            running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "            # Decay probability of taking random action\n",
    "            epsilon -= epsilon_interval / epsilon_greedy_episodes\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Calculate expected value from rewards\n",
    "            # - At each timestep what was the total reward received after that timestep\n",
    "            # - Rewards in the past are discounted by multiplying them with gamma\n",
    "            # - These are the labels for our critic\n",
    "            returns = []\n",
    "            discounted_sum = 0\n",
    "            for r in rewards_history[::-1]:\n",
    "                discounted_sum = r + gamma * discounted_sum\n",
    "                returns.insert(0, discounted_sum)\n",
    "\n",
    "            # Normalize\n",
    "            returns = np.array(returns)\n",
    "            returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "            returns = returns.tolist()\n",
    "\n",
    "            # Calculating loss values to update our network\n",
    "            history = zip(action_probs_history, critic_value_history, returns)\n",
    "            actor_losses = []\n",
    "            critic_losses = []\n",
    "            for log_prob, value, ret in history:\n",
    "                # At this point in history, the critic estimated that we would get a\n",
    "                # total reward = `value` in the future. We took an action with log probability\n",
    "                # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                # The actor must be updated so that it predicts an action that leads to\n",
    "                # high rewards (compared to critic's estimate) with high probability.\n",
    "                diff = ret - value\n",
    "                actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                # The critic must be updated so that it predicts a better estimate of\n",
    "                # the future rewards.\n",
    "                critic_losses.append(\n",
    "                    loss_function(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                )\n",
    "\n",
    "            # Backpropagation\n",
    "            loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # End of tape\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "        action_history.clear()\n",
    "        state_history.clear()\n",
    "        state_next_history.clear()\n",
    "        done_history.clear()\n",
    "\n",
    "        # Log details\n",
    "        episode_count += 1 \n",
    "        print(f\"episode {episode_count} finished (frame {total_frame_count}) - running reward: last state: {state_name}, {running_reward:.2f}, episode reward: {episode_reward:.2f}, actor loss = {np.mean(actor_losses)}, critic loss: {np.mean(critic_losses)}\")\n",
    "\n",
    "        # end of end of episode here\n",
    "\n",
    "# Earlier scrap based on Q-learning\n",
    "'''\n",
    "    # Update every fourth frame and once batch size is over 32\n",
    "    if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "        # Get indices of samples for replay buffers\n",
    "        indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "        # Using list comprehension to sample from replay buffer\n",
    "        state_sample = np.array([state_history[i] for i in indices])\n",
    "        state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "        rewards_sample = [rewards_history[i] for i in indices]\n",
    "        action_sample = [action_history[i] for i in indices]\n",
    "        done_sample = tf.convert_to_tensor(\n",
    "            [float(done_history[i]) for i in indices]\n",
    "        )\n",
    "\n",
    "        # Build the updated Q-values for the sampled future states\n",
    "        # Use the target model for stability\n",
    "        future_rewards = model_target.predict(state_next_sample)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1\n",
    "        )\n",
    "\n",
    "        # If final frame set the last value to -1\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "        # Create a mask so we only calculate loss on the updated Q-values\n",
    "        # updated mask to just be probabilities\n",
    "        masks = tf.convert_to_tensor(action_sample, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = model(state_sample)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if frame_count % update_target_network == 0:\n",
    "        # update the the target network with new weights\n",
    "        model_target.set_weights(model.get_weights())\n",
    "        # Log details\n",
    "        template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "        print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "    # Limit the state and reward history\n",
    "    if len(rewards_history) > max_memory_length:\n",
    "        del rewards_history[:1]\n",
    "        del state_history[:1]\n",
    "        del state_next_history[:1]\n",
    "        del action_history[:1]\n",
    "        del done_history[:1]\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "    print(\"episode: {}, total reward: {:.2f}\".format(episode_count, episode_reward))\n",
    "\n",
    "    if episode_count >= 1000:\n",
    "        model.save('model/test_model')\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f8b0e-bca6-4fd9-8ed8-fc6f001a1d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc95221-f6a2-4168-b9fc-e4f50f7c1ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
